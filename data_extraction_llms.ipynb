{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade --quiet langchain langchain-community langchain-openai chromadb \n",
    "!pip3 install --upgrade --quiet pypdf pandas streamlit python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\source\\python\\structured-rag-pdf\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Import Langchain modules\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Other modules and packages\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\source\\python\\structured-rag-pdf\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the cat get fired from the library? \\n\\nBecause he kept putting books on hold! üòπ \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-c7775f1b-25df-45aa-a3b6-7e732b46ed40-0', usage_metadata={'input_tokens': 7, 'output_tokens': 22, 'total_tokens': 29})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 18 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 0}, page_content='APPLIED COGNITIVE PSYCHOLOGYAppl. Cognit. Psychol.20: 139‚Äì156 (2006)Published online 31 October 2005 in Wiley InterScience(www.interscience.wiley.com) DOI: 10.1002/acp.1178Consequences of Erudite Vernacular Utilized Irrespectiveof Necessity: Problems with Using Long Words NeedlesslyDANIEL M. OPPENHEIMER*Princeton University, USASUMMARYMost texts on writing style encourage authors to avoid overly-complex words. However, a majorityof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to givethe impression of intelligence. This paper explores the extent to which this strategy is effective.Experiments 1‚Äì3 manipulate complexity of texts and Ô¨Ånd a negative relationship between complex-ity and judged intelligence. This relationship held regardless of the quality of the original essay, andirrespective of the participants‚Äô prior expectations of essay quality. The negative impact ofcomplexity was mediated by processing Ô¨Çuency. Experiment 4 directly manipulated Ô¨Çuency andfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5investigated discounting of Ô¨Çuency. When obvious causes for low Ô¨Çuency exist that are not relevantto the judgement at hand, people reduce their reliance on Ô¨Çuency as a cue; in fact, in an effort not tobe inÔ¨Çuenced by the irrelevant source of Ô¨Çuency, they over-compensate and are biased in the oppositedirection. Implications and applications are discussed. Copyright#2005 John Wiley & Sons, Ltd.When it comes to writing, most experts agree that clarity, simplicity and parsimony areideals that authors should strive for. In their classic manual of style, Strunk and White(1979) encourage authors to ‚Äòomit needless words.‚Äô Daryl Bem‚Äôs (1995) guidelines forsubmission toPsychological Bulletinadvise, ‚Äòthe Ô¨Årst step towards clarity is writingsimply.‚Äô Even the APA publication manual (1996) recommends, ‚Äòdirect, declarativesentences with simple common words are usually best.‚ÄôHowever, most of us can likely recall having read papers, either by colleagues orstudents, in which the author appears to be deliberately using overly complex words.Experience suggests that the experts‚Äô advice contrasts with prevailing wisdom on how tosound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polledabout their writing habits, most of them admitted that they had made their writing morecomplex in order to appear smarter. For example, when asked, ‚ÄòHave you ever changed thewords in an academic essay to make the essay sound more valid or intelligent by usingcomplicated language?‚Äô 86.4% of the sample admitted to having done so. Nearly two-thirds answered yes to the question, ‚ÄòWhen you write an essay, do you turn to the thesaurusto choose words that are more complex to give the impression that the content is morevalid or intelligent?‚ÄôCopyright#2005 John Wiley & Sons, Ltd.*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 1}, page_content='There are many plausible reasons that the use of million-dollar words would leadreaders to believe that an author is smart. Intelligence and large vocabularies are positivelycorrelated (Spearman, 1904). Therefore, by displaying a large vocabulary, one may beproviding cues that he or she is intelligent as well. Secondly, writers are assumed to beconforming to the Gricean maxim of manner, ‚Äòavoid obscurity of expression‚Äô (Grice,1975). If authors are believed to be writing as simply as possible, but a text is nonethelesscomplex, a reader might believe that the ideas expressed in that text are also complex,defying all attempts to simplify the language. Further, individuals forced to strugglethrough a complex text might experience dissonance if they believe that the ideas beingconveyed are simple (Festinger, 1957). Thus, individuals might be motivated to perceive adifÔ¨Åcult text as being more worthwhile, thereby justifying the effort of processing.Indeed, there is some evidence that complex vocabulary can be indicative of a moreintelligent author. For example, Pennebaker and King (1999) have shown that thepercentage of long words used in class assignments positively correlates with SATscores and exam grades on both multiple choice and essay tests. However it is difÔ¨Åcultto draw conclusions about the effectivenessof a strategy of complexity from this data.The study did not look at how readers of the texts containing the long words perceivedthe authors‚Äô intelligence. Thus, it is possible that although students using complexvocabularies are objectively very knowledgeable, they might nonetheless be perceivedas being less so.Why might we believe that the experts might be correct in recommending simplicity inwriting? One theory that predicts the effectiveness of straightforward writing is that ofprocessing Ô¨Çuency. Simpler writing is easier to process, and studies have demonstratedthat processing Ô¨Çuency is associated with a variety of positive dimensions. Fluency leadsto higher judgements of truth (Reber & Schwarz, 1999), conÔ¨Ådence (Norwick & Epley,2002), frequency (Tversky & Kahneman, 1973), fame (Jacoby, Kelley, Brown, &Jasechko, 1989), and even liking (Reber, Winkielman, & Schwarz, 1998). Furthermore,the effects of Ô¨Çuency are strongest when the Ô¨Çuency is discrepant‚Äîwhen the amount ofexperienced Ô¨Çuency is surprising (Whittlesea & Williams, 2001a, 2001b). As such, itwould not be surprising if the lower Ô¨Çuency of overly complex texts caused readers to havenegative evaluations of those texts and the associated authors, especially if the complexitywas unnecessary and thus surprising readers with the relative disÔ¨Çuency of the text.Both the experts and prevailing wisdom present plausible views, but which (if either) iscorrect? The present paper provides an empirical investigation of the strategy of complex-ity, and Ô¨Ånds such a strategy to be unsuccessful. Five studies demonstrate that the loss ofÔ¨Çuency due to needless complexity in a text negatively impacts raters‚Äô assessments of thetext‚Äôs authors.EXPERIMENT 1Experiment 1 aimed to answer several simple questions. First, does increasing thecomplexity of text succeed in making the author appear more intelligent? Second, towhat extent does the success of this strategy depend on the quality of the original, simplerwriting? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss ofÔ¨Çuency? To answer these questions, graduate school admission essays were made morecomplex by substituting some of the original words with their longest applicable thesaurusentries.140D. M. Oppenheimer\\nCopyright#2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 2}, page_content='While word length is not perfectly interchangeable with sentence complexity‚Äîforexample, complexity can come from grammatical structure or infrequent words aswell‚Äîit is a useful proxy. Using length as a manipulation of complexity allows for asimple, easily replicable word replacement algorithm. By keeping content constant andvarying the complexity of vocabulary, it was possible to investigate the effectiveness ofcomplexity.Participants and procedureSeventy-one Stanford University undergraduates participated to fulÔ¨Ål part of a courserequirement. The survey was included in a packet of unrelated one-page questionnaires.Packets were distributed in class, and participants were given a week to complete the entirepacket.Stimuli and designSix personal statements for admissions to graduate studies in English Literature weredownloaded from writing improvement websites. The essays varied greatly both in contentand quality of writing. Logical excerpts ranging from 138 to 253 words in length were thentaken from each essay. A ‚Äòhighly complex‚Äô version of each excerpt was prepared byreplacing every noun, verb and adjective with its longest entry in the Microsoft Word 2000thesaurus. Words that were longer than any thesaurus entry, were not listed in thethesaurus, or for which there was no entry with the same linguistic sense were notreplaced. If two entries were of the same length, the replacement was chosen alphabe-tically. When necessary, minor modiÔ¨Åcations were made to the essay to maintain thegrammatical structure of a sentence (e.g. replacing ‚Äòan‚Äô with ‚Äòa‚Äô for replacement wordsbeginning with consonants). A ‚Äòmoderately complex‚Äô version of each excerpt was createdusing the same algorithm as above, except replacing only every third applicable word.Examples of the stimuli can be found in the appendix.Each participant received only one excerpt. Participants were informed that the excerptcame from a personal statement for graduate study in the Stanford English department.They were instructed to read the passage, decide whether or not to accept the applicant,and rate their conÔ¨Ådence in their decision on a 7-point scale.1They were then asked howdifÔ¨Åcult the passage was to understand, also on a seven-point scale.ResultsThe data of one participant was discarded due to an illegible answer. Analysis of themanipulation check showed that more complex texts were more difÔ¨Åcult to read. (x¬º2.9,4.0 and 4.3 for simple, moderately complex and highly complex, respectively). Thesedifferences were reliable,F(2, 68)¬º4.46,p<0.05, Cohen‚Äôsf¬º0.18. For other analyses,acceptance ratings (√æ1 for accept,/C01 for reject) were multiplied by conÔ¨Ådence ratings tocreate a/C07 to 7 scale of admission conÔ¨Ådence. Level of complexity had a reliableinÔ¨Çuence on admission conÔ¨Ådence ratings,F(2, 70)¬º2.46,p<0.05, Cohen‚Äôsf¬º0.12.1With the exception of the dichotomous admissions decision, all dependent measures reported in this paper areseven point scales ranging from 1¬º‚Äònot at all‚Äô to 7¬º‚Äòvery‚Äô.Problems with long words141\\nCopyright#2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                            chunk_overlap=200,\n",
    "                                            length_function=len,\n",
    "                                            separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "chunks = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 0}, page_content='APPLIED COGNITIVE PSYCHOLOGYAppl. Cognit. Psychol.20: 139‚Äì156 (2006)Published online 31 October 2005 in Wiley InterScience(www.interscience.wiley.com) DOI: 10.1002/acp.1178Consequences of Erudite Vernacular Utilized Irrespectiveof Necessity: Problems with Using Long Words NeedlesslyDANIEL M. OPPENHEIMER*Princeton University, USASUMMARYMost texts on writing style encourage authors to avoid overly-complex words. However, a majorityof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to givethe impression of intelligence. This paper explores the extent to which this strategy is effective.Experiments 1‚Äì3 manipulate complexity of texts and Ô¨Ånd a negative relationship between complex-ity and judged intelligence. This relationship held regardless of the quality of the original essay, andirrespective of the participants‚Äô prior expectations of essay quality. The negative impact ofcomplexity was mediated by processing Ô¨Çuency. Experiment 4 directly manipulated Ô¨Çuency andfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5investigated discounting of Ô¨Çuency. When obvious causes for low Ô¨Çuency exist that are not relevantto the judgement at hand, people reduce their reliance on Ô¨Çuency as a cue; in fact, in an effort not tobe inÔ¨Çuenced by the irrelevant source of Ô¨Çuency, they over-compensate and are biased in the oppositedirection. Implications and applications are discussed. Copyright#2005 John Wiley &'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 0}, page_content='an effort not tobe inÔ¨Çuenced by the irrelevant source of Ô¨Çuency, they over-compensate and are biased in the oppositedirection. Implications and applications are discussed. Copyright#2005 John Wiley & Sons, Ltd.When it comes to writing, most experts agree that clarity, simplicity and parsimony areideals that authors should strive for. In their classic manual of style, Strunk and White(1979) encourage authors to ‚Äòomit needless words.‚Äô Daryl Bem‚Äôs (1995) guidelines forsubmission toPsychological Bulletinadvise, ‚Äòthe Ô¨Årst step towards clarity is writingsimply.‚Äô Even the APA publication manual (1996) recommends, ‚Äòdirect, declarativesentences with simple common words are usually best.‚ÄôHowever, most of us can likely recall having read papers, either by colleagues orstudents, in which the author appears to be deliberately using overly complex words.Experience suggests that the experts‚Äô advice contrasts with prevailing wisdom on how tosound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polledabout their writing habits, most of them admitted that they had made their writing morecomplex in order to appear smarter. For example, when asked, ‚ÄòHave you ever changed thewords in an academic essay to make the essay sound more valid or intelligent by usingcomplicated language?‚Äô 86.4% of the sample admitted to having done so. Nearly two-thirds answered yes to the question, ‚ÄòWhen you write an essay, do you turn to the thesaurusto choose words that are more complex'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 0}, page_content='language?‚Äô 86.4% of the sample admitted to having done so. Nearly two-thirds answered yes to the question, ‚ÄòWhen you write an essay, do you turn to the thesaurusto choose words that are more complex to give the impression that the content is morevalid or intelligent?‚ÄôCopyright#2005 John Wiley & Sons, Ltd.*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 1}, page_content='There are many plausible reasons that the use of million-dollar words would leadreaders to believe that an author is smart. Intelligence and large vocabularies are positivelycorrelated (Spearman, 1904). Therefore, by displaying a large vocabulary, one may beproviding cues that he or she is intelligent as well. Secondly, writers are assumed to beconforming to the Gricean maxim of manner, ‚Äòavoid obscurity of expression‚Äô (Grice,1975). If authors are believed to be writing as simply as possible, but a text is nonethelesscomplex, a reader might believe that the ideas expressed in that text are also complex,defying all attempts to simplify the language. Further, individuals forced to strugglethrough a complex text might experience dissonance if they believe that the ideas beingconveyed are simple (Festinger, 1957). Thus, individuals might be motivated to perceive adifÔ¨Åcult text as being more worthwhile, thereby justifying the effort of processing.Indeed, there is some evidence that complex vocabulary can be indicative of a moreintelligent author. For example, Pennebaker and King (1999) have shown that thepercentage of long words used in class assignments positively correlates with SATscores and exam grades on both multiple choice and essay tests. However it is difÔ¨Åcultto draw conclusions about the effectivenessof a strategy of complexity from this data.The study did not look at how readers of the texts containing the long words perceivedthe authors‚Äô intelligence. Thus, it is'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 1}, page_content='about the effectivenessof a strategy of complexity from this data.The study did not look at how readers of the texts containing the long words perceivedthe authors‚Äô intelligence. Thus, it is possible that although students using complexvocabularies are objectively very knowledgeable, they might nonetheless be perceivedas being less so.Why might we believe that the experts might be correct in recommending simplicity inwriting? One theory that predicts the effectiveness of straightforward writing is that ofprocessing Ô¨Çuency. Simpler writing is easier to process, and studies have demonstratedthat processing Ô¨Çuency is associated with a variety of positive dimensions. Fluency leadsto higher judgements of truth (Reber & Schwarz, 1999), conÔ¨Ådence (Norwick & Epley,2002), frequency (Tversky & Kahneman, 1973), fame (Jacoby, Kelley, Brown, &Jasechko, 1989), and even liking (Reber, Winkielman, & Schwarz, 1998). Furthermore,the effects of Ô¨Çuency are strongest when the Ô¨Çuency is discrepant‚Äîwhen the amount ofexperienced Ô¨Çuency is surprising (Whittlesea & Williams, 2001a, 2001b). As such, itwould not be surprising if the lower Ô¨Çuency of overly complex texts caused readers to havenegative evaluations of those texts and the associated authors, especially if the complexitywas unnecessary and thus surprising readers with the relative disÔ¨Çuency of the text.Both the experts and prevailing wisdom present plausible views, but which (if either) iscorrect? The present paper provides an empirical'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 1}, page_content='surprising readers with the relative disÔ¨Çuency of the text.Both the experts and prevailing wisdom present plausible views, but which (if either) iscorrect? The present paper provides an empirical investigation of the strategy of complex-ity, and Ô¨Ånds such a strategy to be unsuccessful. Five studies demonstrate that the loss ofÔ¨Çuency due to needless complexity in a text negatively impacts raters‚Äô assessments of thetext‚Äôs authors.EXPERIMENT 1Experiment 1 aimed to answer several simple questions. First, does increasing thecomplexity of text succeed in making the author appear more intelligent? Second, towhat extent does the success of this strategy depend on the quality of the original, simplerwriting? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss ofÔ¨Çuency? To answer these questions, graduate school admission essays were made morecomplex by substituting some of the original words with their longest applicable thesaurusentries.140D. M. Oppenheimer'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 1}, page_content='Copyright#2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 2}, page_content='While word length is not perfectly interchangeable with sentence complexity‚Äîforexample, complexity can come from grammatical structure or infrequent words aswell‚Äîit is a useful proxy. Using length as a manipulation of complexity allows for asimple, easily replicable word replacement algorithm. By keeping content constant andvarying the complexity of vocabulary, it was possible to investigate the effectiveness ofcomplexity.Participants and procedureSeventy-one Stanford University undergraduates participated to fulÔ¨Ål part of a courserequirement. The survey was included in a packet of unrelated one-page questionnaires.Packets were distributed in class, and participants were given a week to complete the entirepacket.Stimuli and designSix personal statements for admissions to graduate studies in English Literature weredownloaded from writing improvement websites. The essays varied greatly both in contentand quality of writing. Logical excerpts ranging from 138 to 253 words in length were thentaken from each essay. A ‚Äòhighly complex‚Äô version of each excerpt was prepared byreplacing every noun, verb and adjective with its longest entry in the Microsoft Word 2000thesaurus. Words that were longer than any thesaurus entry, were not listed in thethesaurus, or for which there was no entry with the same linguistic sense were notreplaced. If two entries were of the same length, the replacement was chosen alphabe-tically. When necessary, minor modiÔ¨Åcations were made to the essay to maintain'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 2}, page_content='the same linguistic sense were notreplaced. If two entries were of the same length, the replacement was chosen alphabe-tically. When necessary, minor modiÔ¨Åcations were made to the essay to maintain thegrammatical structure of a sentence (e.g. replacing ‚Äòan‚Äô with ‚Äòa‚Äô for replacement wordsbeginning with consonants). A ‚Äòmoderately complex‚Äô version of each excerpt was createdusing the same algorithm as above, except replacing only every third applicable word.Examples of the stimuli can be found in the appendix.Each participant received only one excerpt. Participants were informed that the excerptcame from a personal statement for graduate study in the Stanford English department.They were instructed to read the passage, decide whether or not to accept the applicant,and rate their conÔ¨Ådence in their decision on a 7-point scale.1They were then asked howdifÔ¨Åcult the passage was to understand, also on a seven-point scale.ResultsThe data of one participant was discarded due to an illegible answer. Analysis of themanipulation check showed that more complex texts were more difÔ¨Åcult to read. (x¬º2.9,4.0 and 4.3 for simple, moderately complex and highly complex, respectively). Thesedifferences were reliable,F(2, 68)¬º4.46,p<0.05, Cohen‚Äôsf¬º0.18. For other analyses,acceptance ratings (√æ1 for accept,/C01 for reject) were multiplied by conÔ¨Ådence ratings tocreate a/C07 to 7 scale of admission conÔ¨Ådence. Level of complexity had a reliableinÔ¨Çuence on admission conÔ¨Ådence ratings,F(2,'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 2}, page_content='(√æ1 for accept,/C01 for reject) were multiplied by conÔ¨Ådence ratings tocreate a/C07 to 7 scale of admission conÔ¨Ådence. Level of complexity had a reliableinÔ¨Çuence on admission conÔ¨Ådence ratings,F(2, 70)¬º2.46,p<0.05, Cohen‚Äôsf¬º0.12.1With the exception of the dichotomous admissions decision, all dependent measures reported in this paper areseven point scales ranging from 1¬º‚Äònot at all‚Äô to 7¬º‚Äòvery‚Äô.Problems with long words141'),\n",
       " Document(metadata={'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page': 2}, page_content='Copyright#2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\source\\python\\structured-rag-pdf\\venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    # embeddings = OpenAIEmbeddings(\n",
    "    #     model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY\n",
    "    # )\n",
    "\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector = embedding_function.embed_query(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09987300105413044}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\", \n",
    "                            embeddings=embedding_function)\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Amsterdam\", reference=\"coffeeshop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09696432595303928}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_strings(prediction=\"Paris\", reference=\"coffeeshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "\n",
    "    # Create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    # Ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    unique_chunks = [] \n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        if id not in unique_ids:       \n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk) \n",
    "\n",
    "    # Create a new Chroma database from the documents\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function, \n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldt\\AppData\\Local\\Temp\\ipykernel_46816\\2260934871.py:24: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Create vectorstore\n",
    "vectorstore = create_vectorstore(chunks=chunks, \n",
    "                                 embedding_function=embedding_function, \n",
    "                                 vectorstore_path=\"vectorstore_chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1d353b3bdd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query for relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldt\\AppData\\Local\\Temp\\ipykernel_46816\\2183088283.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"vectorstore_chroma\", embedding_function=embedding_function)\n"
     ]
    }
   ],
   "source": [
    "# Load vectorstore\n",
    "vectorstore = Chroma(persist_directory=\"vectorstore_chroma\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 1, 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf'}, page_content='Copyright#2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)'),\n",
       " Document(metadata={'page': 2, 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf'}, page_content='the same linguistic sense were notreplaced. If two entries were of the same length, the replacement was chosen alphabe-tically. When necessary, minor modiÔ¨Åcations were made to the essay to maintain thegrammatical structure of a sentence (e.g. replacing ‚Äòan‚Äô with ‚Äòa‚Äô for replacement wordsbeginning with consonants). A ‚Äòmoderately complex‚Äô version of each excerpt was createdusing the same algorithm as above, except replacing only every third applicable word.Examples of the stimuli can be found in the appendix.Each participant received only one excerpt. Participants were informed that the excerptcame from a personal statement for graduate study in the Stanford English department.They were instructed to read the passage, decide whether or not to accept the applicant,and rate their conÔ¨Ådence in their decision on a 7-point scale.1They were then asked howdifÔ¨Åcult the passage was to understand, also on a seven-point scale.ResultsThe data of one participant was discarded due to an illegible answer. Analysis of themanipulation check showed that more complex texts were more difÔ¨Åcult to read. (x¬º2.9,4.0 and 4.3 for simple, moderately complex and highly complex, respectively). Thesedifferences were reliable,F(2, 68)¬º4.46,p<0.05, Cohen‚Äôsf¬º0.18. For other analyses,acceptance ratings (√æ1 for accept,/C01 for reject) were multiplied by conÔ¨Ådence ratings tocreate a/C07 to 7 scale of admission conÔ¨Ådence. Level of complexity had a reliableinÔ¨Çuence on admission conÔ¨Ådence ratings,F(2,'),\n",
       " Document(metadata={'page': 0, 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf'}, page_content='APPLIED COGNITIVE PSYCHOLOGYAppl. Cognit. Psychol.20: 139‚Äì156 (2006)Published online 31 October 2005 in Wiley InterScience(www.interscience.wiley.com) DOI: 10.1002/acp.1178Consequences of Erudite Vernacular Utilized Irrespectiveof Necessity: Problems with Using Long Words NeedlesslyDANIEL M. OPPENHEIMER*Princeton University, USASUMMARYMost texts on writing style encourage authors to avoid overly-complex words. However, a majorityof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to givethe impression of intelligence. This paper explores the extent to which this strategy is effective.Experiments 1‚Äì3 manipulate complexity of texts and Ô¨Ånd a negative relationship between complex-ity and judged intelligence. This relationship held regardless of the quality of the original essay, andirrespective of the participants‚Äô prior expectations of essay quality. The negative impact ofcomplexity was mediated by processing Ô¨Çuency. Experiment 4 directly manipulated Ô¨Çuency andfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5investigated discounting of Ô¨Çuency. When obvious causes for low Ô¨Çuency exist that are not relevantto the judgement at hand, people reduce their reliance on Ô¨Çuency as a cue; in fact, in an effort not tobe inÔ¨Çuenced by the irrelevant source of Ô¨Çuency, they over-compensate and are biased in the oppositedirection. Implications and applications are discussed. Copyright#2005 John Wiley &'),\n",
       " Document(metadata={'page': 2, 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf'}, page_content='(√æ1 for accept,/C01 for reject) were multiplied by conÔ¨Ådence ratings tocreate a/C07 to 7 scale of admission conÔ¨Ådence. Level of complexity had a reliableinÔ¨Çuence on admission conÔ¨Ådence ratings,F(2, 70)¬º2.46,p<0.05, Cohen‚Äôsf¬º0.12.1With the exception of the dichotomous admissions decision, all dependent measures reported in this paper areseven point scales ranging from 1¬º‚Äònot at all‚Äô to 7¬º‚Äòvery‚Äô.Problems with long words141')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create retriever and get relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "relevant_chunks = retriever.invoke(\"What is the title of the paper?\")\n",
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. DON'T MAKE UP ANYTHING.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer\n",
      "the question. If you don't know the answer, say that you\n",
      "don't know. DON'T MAKE UP ANYTHING.\n",
      "\n",
      "Copyright#2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)\n",
      "\n",
      "---\n",
      "\n",
      "the same linguistic sense were notreplaced. If two entries were of the same length, the replacement was chosen alphabe-tically. When necessary, minor modiÔ¨Åcations were made to the essay to maintain thegrammatical structure of a sentence (e.g. replacing ‚Äòan‚Äô with ‚Äòa‚Äô for replacement wordsbeginning with consonants). A ‚Äòmoderately complex‚Äô version of each excerpt was createdusing the same algorithm as above, except replacing only every third applicable word.Examples of the stimuli can be found in the appendix.Each participant received only one excerpt. Participants were informed that the excerptcame from a personal statement for graduate study in the Stanford English department.They were instructed to read the passage, decide whether or not to accept the applicant,and rate their conÔ¨Ådence in their decision on a 7-point scale.1They were then asked howdifÔ¨Åcult the passage was to understand, also on a seven-point scale.ResultsThe data of one participant was discarded due to an illegible answer. Analysis of themanipulation check showed that more complex texts were more difÔ¨Åcult to read. (x¬º2.9,4.0 and 4.3 for simple, moderately complex and highly complex, respectively). Thesedifferences were reliable,F(2, 68)¬º4.46,p<0.05, Cohen‚Äôsf¬º0.18. For other analyses,acceptance ratings (√æ1 for accept,/C01 for reject) were multiplied by conÔ¨Ådence ratings tocreate a/C07 to 7 scale of admission conÔ¨Ådence. Level of complexity had a reliableinÔ¨Çuence on admission conÔ¨Ådence ratings,F(2,\n",
      "\n",
      "---\n",
      "\n",
      "APPLIED COGNITIVE PSYCHOLOGYAppl. Cognit. Psychol.20: 139‚Äì156 (2006)Published online 31 October 2005 in Wiley InterScience(www.interscience.wiley.com) DOI: 10.1002/acp.1178Consequences of Erudite Vernacular Utilized Irrespectiveof Necessity: Problems with Using Long Words NeedlesslyDANIEL M. OPPENHEIMER*Princeton University, USASUMMARYMost texts on writing style encourage authors to avoid overly-complex words. However, a majorityof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to givethe impression of intelligence. This paper explores the extent to which this strategy is effective.Experiments 1‚Äì3 manipulate complexity of texts and Ô¨Ånd a negative relationship between complex-ity and judged intelligence. This relationship held regardless of the quality of the original essay, andirrespective of the participants‚Äô prior expectations of essay quality. The negative impact ofcomplexity was mediated by processing Ô¨Çuency. Experiment 4 directly manipulated Ô¨Çuency andfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5investigated discounting of Ô¨Çuency. When obvious causes for low Ô¨Çuency exist that are not relevantto the judgement at hand, people reduce their reliance on Ô¨Çuency as a cue; in fact, in an effort not tobe inÔ¨Çuenced by the irrelevant source of Ô¨Çuency, they over-compensate and are biased in the oppositedirection. Implications and applications are discussed. Copyright#2005 John Wiley &\n",
      "\n",
      "---\n",
      "\n",
      "(√æ1 for accept,/C01 for reject) were multiplied by conÔ¨Ådence ratings tocreate a/C07 to 7 scale of admission conÔ¨Ådence. Level of complexity had a reliableinÔ¨Çuence on admission conÔ¨Ådence ratings,F(2, 70)¬º2.46,p<0.05, Cohen‚Äôsf¬º0.12.1With the exception of the dichotomous admissions decision, all dependent measures reported in this paper areseven point scales ranging from 1¬º‚Äònot at all‚Äô to 7¬º‚Äòvery‚Äô.Problems with long words141\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the title of the paper?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate context text\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, \n",
    "                                question=\"What is the title of the paper?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The title of the paper is \"Consequences of Erudite Vernacular Utilized Irrespective of Necessity: Problems with Using Long Words Needlessly\". \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'LOW', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-532c1b7c-52b8-454e-a95f-38835b067300-0', usage_metadata={'input_tokens': 938, 'output_tokens': 29, 'total_tokens': 967})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The title of the paper is \"Consequences of Erudite Vernacular Utilized Irrespective of Necessity: Problems with Using Long Words Needlessly\". \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'LOW', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-51614c05-fb59-4b35-b3d4-e604bd9d32b2-0', usage_metadata={'input_tokens': 930, 'output_tokens': 29, 'total_tokens': 959})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "        )\n",
    "rag_chain.invoke(\"What's the title of this paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate structured responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithSources(BaseModel):\n",
    "    \"\"\"An answer to the question, with sources and reasoning.\"\"\"\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "    \n",
    "class ExtractedInfo2(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    title: AnswerWithSources\n",
    "    summary: AnswerWithSources\n",
    "    publication_year: AnswerWithSources\n",
    "    paper_authors: AnswerWithSources\n",
    "\n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    summary: str = Field(description=\"Summary of the paper\")\n",
    "    publication_year: str = Field(description=\"Year of publication of the paper\")\n",
    "    paper_authors: str = Field(description=\"Name of the authors of the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtractedInfo(title='Consequences of Erudite Vernacular Utilized Irrespective of Necessity: Problems with Using Long Words Needlessly', summary='Most texts on writing style encourage authors to avoid overly-complex words. However, a majority of undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give the impression of intelligence. This paper explores the extent to which this strategy is effective. Experiments 1‚Äì3 manipulate complexity of texts and Ô¨Ånd a negative relationship between complex-ity and judged intelligence. This relationship held regardless of the quality of the original essay, and irrespective of the participants‚Äô prior expectations of essay quality. The negative impact of complexity was mediated by processing Ô¨Çuency. Experiment 4 directly manipulated Ô¨Çuency and found that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5 investigated discounting of Ô¨Çuency. When obvious causes for low Ô¨Çuency exist that are not relevant to the judgement at hand, people reduce their reliance on Ô¨Çuency as a cue; in fact, in an effort not to be inÔ¨Çuenced by the irrelevant source of Ô¨Çuency, they over-compensate and are biased in the opposite direction. Implications and applications are discussed.', publication_year='2006', paper_authors='DANIEL M. OPPENHEIMER')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm.with_structured_output(ExtractedInfo)\n",
    "        )\n",
    "\n",
    "rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractedInfo(title='Consequences of Erudite Vernacular Utilized Irrespectiveof Necessity: Problems with Using Long Words Needlessly', summary='Most texts on writing style encourage authors to avoid overly-complex words. However, a majorityof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to givethe impression of intelligence. This paper explores the extent to which this strategy is effective.Experiments 1‚Äì3 manipulate complexity of texts and Ô¨Ånd a negative relationship between complex-ity and judged intelligence. This relationship held regardless of the quality of the original essay, andirrespective of the participants‚Äô prior expectations of essay quality. The negative impact ofcomplexity was mediated by processing Ô¨Çuency. Experiment 4 directly manipulated Ô¨Çuency andfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5investigated discounting of Ô¨Çuency. When obvious causes for low Ô¨Çuency exist that are not relevantto the judgement at hand, people reduce their reliance on Ô¨Çuency as a cue; in fact, in an effort not tobe inÔ¨Çuenced by the irrelevant source of Ô¨Çuency, they over-compensate and are biased in the oppositedirection. Implications and applications are discussed.', publication_year='2006', paper_authors='DANIEL M. OPPENHEIMER')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([structured_response.dict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>paper_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consequences of Erudite Vernacular Utilized Ir...</td>\n",
       "      <td>Most texts on writing style encourage authors ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>DANIEL M. OPPENHEIMER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Consequences of Erudite Vernacular Utilized Ir...   \n",
       "\n",
       "                                             summary publication_year  \\\n",
       "0  Most texts on writing style encourage authors ...             2006   \n",
       "\n",
       "           paper_authors  \n",
       "0  DANIEL M. OPPENHEIMER  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform response into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_summary</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>paper_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>Consequences of Erudite Vernacular Utilized Ir...</td>\n",
       "      <td>The paper explores the negative relationship b...</td>\n",
       "      <td>2006</td>\n",
       "      <td>Daniel M. Oppenheimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>Copyright#2005 John Wiley &amp; Sons, Ltd. Appl. C...</td>\n",
       "      <td>Most texts on writing style encourage authors ...</td>\n",
       "      <td>Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)</td>\n",
       "      <td>Correspondence to: D. M. Oppenheimer, Departme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoning</th>\n",
       "      <td>The title is explicitly mentioned at the begin...</td>\n",
       "      <td>The summary is derived from the overall conten...</td>\n",
       "      <td>The publication year is indicated in the citat...</td>\n",
       "      <td>The author‚Äôs name is provided in the correspon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paper_title  \\\n",
       "answer     Consequences of Erudite Vernacular Utilized Ir...   \n",
       "source     Copyright#2005 John Wiley & Sons, Ltd. Appl. C...   \n",
       "reasoning  The title is explicitly mentioned at the begin...   \n",
       "\n",
       "                                               paper_summary  \\\n",
       "answer     The paper explores the negative relationship b...   \n",
       "source     Most texts on writing style encourage authors ...   \n",
       "reasoning  The summary is derived from the overall conten...   \n",
       "\n",
       "                                            publication_year  \\\n",
       "answer                                                  2006   \n",
       "source             Appl. Cognit. Psychol. 20: 139‚Äì156 (2006)   \n",
       "reasoning  The publication year is indicated in the citat...   \n",
       "\n",
       "                                               paper_authors  \n",
       "answer                                 Daniel M. Oppenheimer  \n",
       "source     Correspondence to: D. M. Oppenheimer, Departme...  \n",
       "reasoning  The author‚Äôs name is provided in the correspon...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n",
    "df = pd.DataFrame([structured_response.dict()])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
